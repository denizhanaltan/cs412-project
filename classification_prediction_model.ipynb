{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import regex  # Use 'regex' module instead of 're' for better Unicode support\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from torch.nn.functional import softmax\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 120 labels with no input data.\n"
     ]
    }
   ],
   "source": [
    "## GET LABELS\n",
    "train_classification_df = pd.read_csv(\"training-dataset-labels.csv\")\n",
    "\n",
    "# Rename columns to match the original code's structure\n",
    "train_classification_df = train_classification_df.rename(columns={'username': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unify labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "\n",
    "# Convert the DataFrame into a dictionary mapping user_id to category\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
    "\n",
    "## GET INPUT DATA\n",
    "train_data_path = \"training-dataset.jsonl.gz\"\n",
    "\n",
    "username2posts_train = dict()\n",
    "username2profile_train = dict()\n",
    "\n",
    "username2posts_test = dict()\n",
    "username2profile_test = dict()\n",
    "\n",
    "\n",
    "with gzip.open(train_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "    profile = sample[\"profile\"]\n",
    "    username = profile[\"username\"]\n",
    "    if username in username2_category:\n",
    "      # train data info\n",
    "      username2posts_train[username] = sample[\"posts\"]\n",
    "      username2profile_train[username] = profile\n",
    "\n",
    "\n",
    "    else:\n",
    "      # it is test data info\n",
    "      username2posts_test[username] = sample[\"posts\"]\n",
    "      username2profile_test[username] = profile\n",
    "\n",
    "# Remove labels with no inputs username2profile_train.loc[\"username\"]:\n",
    "len_before = len(username2_category)\n",
    "username2_category = {k: v for k, v in username2_category.items() if k in username2posts_train}\n",
    "len_after = len(username2_category)\n",
    "print(f\"Removed {len_before - len_after} labels with no input data.\")\n",
    "\n",
    "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
    "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 usernames from 'test-classification-round3.dat'.\n"
     ]
    }
   ],
   "source": [
    "# Load the usernames from \"test-classification-round3.dat\"\n",
    "round_usernames = []\n",
    "with open(\"test-classification-round3.dat\", \"r\") as f:\n",
    "  for line in f:\n",
    "    round_usernames.append(line.strip())\n",
    "  \n",
    "len_before = len(round_usernames)\n",
    "print(f\"Loaded {len_before} usernames from 'test-classification-round3.dat'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 713 usernames in test data and 287 usernames in train data.\n",
      "Found 0 usernames in neither train nor test data.\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "train_count = 0\n",
    "not_found = 0\n",
    "\n",
    "round_usernames_to_classify = []\n",
    "round_usernames_classified = {}\n",
    "\n",
    "for username in round_usernames:\n",
    "  if username in test_profile_df.username.values:\n",
    "    round_usernames_to_classify.append(username)\n",
    "    test_count += 1\n",
    "  if username in train_profile_df.username.values:\n",
    "    round_usernames_classified[username] = username2_category[username]\n",
    "    train_count += 1\n",
    "\n",
    "print(f\"Found {test_count} usernames in test data and {train_count} usernames in train data.\")\n",
    "print(f\"Found {not_found} usernames in neither train nor test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    # 1) Normalize the text to NFC so that Turkish characters are properly composed\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # 2) Remove or replace escape sequences (both actual and literal)\n",
    "    #    This handles actual newlines (\\n) and also the literal two-character sequence '\\n'\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = regex.sub(r'\\\\n', ' ', text)  # if you have literal \"\\n\" in your data\n",
    "\n",
    "    # 3) Remove URLs (http, https, www)\n",
    "    text = regex.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # 4) Remove extra spaces\n",
    "    text = regex.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "username2posts_test_preprocessed = dict()\n",
    "for username, posts in username2posts_test.items():\n",
    "    username2posts_test_preprocessed[username] = [\n",
    "        {**post, \"caption\": preprocess_text(post[\"caption\"])} \n",
    "        for post in posts if \"caption\" in post and post[\"caption\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "round = []\n",
    "\n",
    "for username in round_usernames_to_classify:\n",
    "    if username not in username2posts_test_preprocessed:\n",
    "        print(f\"Warning: {username} has no posts with captions.\")\n",
    "    else:\n",
    "        posts = username2posts_test_preprocessed[username]\n",
    "        for post in posts:\n",
    "            round.append({\n",
    "                \"username\": username,\n",
    "                \"caption\": post[\"caption\"]\n",
    "            })\n",
    "\n",
    "        if username2profile_test[username][\"biography\"]:\n",
    "            round.append({\"username\": username, \"caption\": preprocess_text(username2profile_test[username][\"biography\"]) })\n",
    "\n",
    "df_round = pd.DataFrame(round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username2posts_train_preprocessed = dict()\n",
    "# for username, posts in username2posts_train.items():\n",
    "#     username2posts_train_preprocessed[username] = [\n",
    "#         {**post, \"caption\": preprocess_text(post[\"caption\"]), \"class\": username2_category[username]} \n",
    "#         for post in posts if \"caption\" in post and post[\"caption\"]\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_train_df = pd.merge(\n",
    "#     train_profile_df,               # has column \"id\"\n",
    "#     train_classification_df,        # has column \"user_id\"\n",
    "#     left_on=\"username\",                   # train_profile_df.id\n",
    "#     right_on=\"user_id\",            # train_classification_df.user_id\n",
    "#     how=\"inner\"                     # only keep matching rows\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions = []\n",
    "\n",
    "# for username, posts in username2posts_train_preprocessed.items():\n",
    "#     for post in posts:\n",
    "#         if \"caption\" in post and post[\"caption\"]:\n",
    "#             captions.append({\"caption\": post[\"caption\"], \"class\": post[\"class\"], \"username\": username})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = ['tech', 'food', 'health and lifestyle', 'travel', 'sports', 'fashion', 'entertainment', 'mom and children', 'art', 'gaming']\n",
    "# label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "# id2label = {i: lbl for i, lbl in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_captions = pd.DataFrame(captions)\n",
    "# df_train_captions[\"label_id\"] = df_train_captions[\"class\"].map(label2id)\n",
    "\n",
    "# dataset = Dataset.from_pandas(df_train_captions[[\"caption\", \"label_id\", \"username\"]])\n",
    "\n",
    "# dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "# train_dataset = dataset_split[\"train\"]\n",
    "# eval_dataset = dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"./checkpoint-9869\"\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(\n",
    "#         examples[\"caption\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=128\n",
    "#     )\n",
    "\n",
    "# def rename_label_column(examples):\n",
    "#     examples[\"labels\"] = examples[\"label_id\"]\n",
    "#     return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "# tokenized_eval_dataset = tokenized_eval_dataset.map(rename_label_column, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract logits and labels\n",
    "# logits = predictions.predictions\n",
    "# labels = predictions.label_ids\n",
    "\n",
    "# # Compute predictions from logits\n",
    "# predicted_labels = torch.argmax(torch.tensor(logits), axis=1)\n",
    "\n",
    "# # Compute accuracy\n",
    "# accuracy = accuracy_metric.compute(predictions=predicted_labels, references=labels)[\"accuracy\"]\n",
    "\n",
    "# # Print the accuracy\n",
    "# print(f\"Accuracy of the model: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"caption\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "def rename_label_column(examples):\n",
    "    examples[\"labels\"] = examples[\"label_id\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['tech', 'food', 'health and lifestyle', 'travel', 'sports', 'fashion', 'entertainment', 'mom and children', 'art', 'gaming']\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for i, lbl in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acd2f212b7f4ee8aeb8e948d863acf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/9mt7fhc54bs1061p9hqr6t6h0000gn/T/ipykernel_59434/3493158275.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, tokenizer=tokenizer)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c83b5413b06484eabe3c8d58e6b609b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3072 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Dataset from df_round\n",
    "ds_infer = Dataset.from_pandas(df_round)\n",
    "\n",
    "# Tokenize\n",
    "ds_infer = ds_infer.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define a Trainer for prediction\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Predict\n",
    "preds = trainer.predict(ds_infer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_ids = np.argmax(preds.predictions, axis=1)\n",
    "df_round[\"predicted_class\"] = [label_list[i] for i in pred_label_ids]\n",
    "\n",
    "post_probs = softmax(torch.tensor(preds.predictions), dim=-1).numpy()\n",
    "df_round[\"probs\"] = list(post_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_probs(probs_list):\n",
    "    # probs_list is a list of arrays, each array is shape [num_labels, ]\n",
    "    # We can stack them and take mean\n",
    "    arr = np.vstack(probs_list)  # shape [num_posts_for_user, num_labels]\n",
    "    return arr.mean(axis=0)      # shape [num_labels,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_round_res = (\n",
    "    df_round\n",
    "    .groupby(\"username\")[\"probs\"]\n",
    "    .apply(lambda x: avg_probs(x))\n",
    "    .reset_index(name=\"avg_probs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_round_res[\"user_predicted_category\"] = df_round_res[\"avg_probs\"].apply(lambda x: label_list[np.argmax(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_usernames_classified.update(df_round_res.set_index(\"username\")[\"user_predicted_category\"].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prediction-classification-round3.json\", \"w\") as f:\n",
    "    json.dump(round_usernames_classified, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
