{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import regex  # Use 'regex' module instead of 're' for better Unicode support\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langdetect import detect, DetectorFactory, LangDetectException\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.nn.functional import softmax\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 120 labels with no input data.\n"
     ]
    }
   ],
   "source": [
    "## GET LABELS\n",
    "train_classification_df = pd.read_csv(\"training-dataset-labels.csv\")\n",
    "\n",
    "# Rename columns to match the original code's structure\n",
    "train_classification_df = train_classification_df.rename(columns={'username': 'user_id', 'label': 'category'})\n",
    "\n",
    "# Unify labels\n",
    "train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "\n",
    "# Convert the DataFrame into a dictionary mapping user_id to category\n",
    "username2_category = train_classification_df.set_index(\"user_id\").to_dict()[\"category\"]\n",
    "\n",
    "## GET INPUT DATA\n",
    "train_data_path = \"training-dataset.jsonl.gz\"\n",
    "\n",
    "username2posts_train = dict()\n",
    "username2profile_train = dict()\n",
    "\n",
    "username2posts_test = dict()\n",
    "username2profile_test = dict()\n",
    "\n",
    "\n",
    "with gzip.open(train_data_path, \"rt\") as fh:\n",
    "  for line in fh:\n",
    "    sample = json.loads(line)\n",
    "    profile = sample[\"profile\"]\n",
    "    username = profile[\"username\"]\n",
    "    if username in username2_category:\n",
    "      # train data info\n",
    "      username2posts_train[username] = sample[\"posts\"]\n",
    "      username2profile_train[username] = profile\n",
    "\n",
    "\n",
    "    else:\n",
    "      # it is test data info\n",
    "      username2posts_test[username] = sample[\"posts\"]\n",
    "      username2profile_test[username] = profile\n",
    "\n",
    "# Remove labels with no inputs username2profile_train.loc[\"username\"]:\n",
    "len_before = len(username2_category)\n",
    "username2_category = {k: v for k, v in username2_category.items() if k in username2posts_train}\n",
    "len_after = len(username2_category)\n",
    "print(f\"Removed {len_before - len_after} labels with no input data.\")\n",
    "\n",
    "train_profile_df = pd.DataFrame(username2profile_train).T.reset_index(drop=True)\n",
    "test_profile_df = pd.DataFrame(username2profile_test).T.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    # 1) Normalize the text to NFC so that Turkish characters are properly composed\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # 2) Remove or replace escape sequences (both actual and literal)\n",
    "    #    This handles actual newlines (\\n) and also the literal two-character sequence '\\n'\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = regex.sub(r'\\\\n', ' ', text)  # if you have literal \"\\n\" in your data\n",
    "\n",
    "    # 3) Remove URLs (http, https, www)\n",
    "    text = regex.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # 4) Remove extra spaces\n",
    "    text = regex.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "username2posts_train_preprocessed = dict()\n",
    "for username, posts in username2posts_train.items():\n",
    "    username2posts_train_preprocessed[username] = [\n",
    "        {**post, \"caption\": preprocess_text(post[\"caption\"]), \"class\": username2_category[username]} \n",
    "        for post in posts if \"caption\" in post and post[\"caption\"]\n",
    "    ]\n",
    "\n",
    "username2posts_test_preprocessed = dict()\n",
    "for username, posts in username2posts_train.items():\n",
    "    username2posts_test_preprocessed[username] = [\n",
    "        {**post, \"caption\": preprocess_text(post[\"caption\"])} \n",
    "        for post in posts if \"caption\" in post and post[\"caption\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a column for post captions from username2posts_train\n",
    "# train_profile_df['captions'] = train_profile_df['username'].apply(\n",
    "#     lambda username: ' '.join(\n",
    "#         [post['caption'] for post in username2posts_train.get(username, []) if 'caption' in post and post['caption']]\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Combine biography and captions\n",
    "# train_profile_df['biography'] = train_profile_df['biography'].fillna('')\n",
    "# train_profile_df['combined_raw_text'] = train_profile_df['biography'] + ' ' + train_profile_df['captions']\n",
    "\n",
    "# # Drop rows with missing combined text\n",
    "# train_profile_df = train_profile_df.dropna(subset=['combined_raw_text'])\n",
    "\n",
    "# def preprocess_text0(text: str):\n",
    "#     # 1) Normalize text so that Turkish casefolding doesn't split into multiple chars\n",
    "#     text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "#     # 2) Lower-case (casefold) the text\n",
    "#     text = text.casefold()\n",
    "\n",
    "#     # Remove URLs\n",
    "#     text = regex.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=regex.MULTILINE)\n",
    "\n",
    "#     # IMPORTANT: Include \\p{M} to allow combining diacritics (otherwise \"i + dot\" can break words)\n",
    "#     # This pattern keeps letters (L), combining marks (M), numbers (N), whitespace, #, @, emoji, etc.\n",
    "#     text = regex.sub(r'[^\\p{L}\\p{M}\\p{N}\\s#@\\p{So}\\p{Sk}\\p{Sm}\\p{Emoji}]+', ' ', text)\n",
    "\n",
    "#     # Remove standalone numbers (optional)\n",
    "#     text = regex.sub(r'\\s\\d+\\s', ' ', text)\n",
    "\n",
    "#     # Remove extra whitespaces\n",
    "#     text = regex.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "#     return text\n",
    "\n",
    "# train_profile_df['combined_processed_text'] = train_profile_df['combined_raw_text'].apply(preprocess_text0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification_df.groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'category'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classification_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of users in the training set: \", len(train_profile_df))\n",
    "# print(\"Number of users in the test set: \", len(test_profile_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['username', 'id', 'full_name', 'biography', 'category_name',\n",
       "       'post_count', 'follower_count', 'following_count',\n",
       "       'is_business_account', 'is_private', 'is_verified',\n",
       "       'highlight_reel_count', 'bio_links', 'entities', 'ai_agent_type',\n",
       "       'fb_profile_biolink', 'restricted_by_viewer', 'country_block',\n",
       "       'eimu_id', 'external_url', 'fbid', 'has_clips',\n",
       "       'hide_like_and_view_counts', 'is_professional_account',\n",
       "       'is_supervision_enabled', 'is_guardian_of_viewer',\n",
       "       'is_supervised_by_viewer', 'is_supervised_user', 'is_embeds_disabled',\n",
       "       'is_joined_recently', 'business_address_json',\n",
       "       'business_contact_method', 'business_email', 'business_phone_number',\n",
       "       'business_category_name', 'overall_category_name', 'category_enum',\n",
       "       'is_verified_by_mv4b', 'is_regulated_c18', 'profile_pic_url',\n",
       "       'should_show_category', 'should_show_public_contacts',\n",
       "       'show_account_transparency_details', 'profile_picture_base64'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_profile_df.columns\n",
    "test_profile_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username2posts_train['sercevdernegi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_profile_df.loc[0, 'combined_raw_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_profile_df.loc[0, 'combined_processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_profile_df.loc[0, 'combined_processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_df = pd.merge(\n",
    "    train_profile_df,               # has column \"id\"\n",
    "    train_classification_df,        # has column \"user_id\"\n",
    "    left_on=\"username\",                   # train_profile_df.id\n",
    "    right_on=\"user_id\",            # train_classification_df.user_id\n",
    "    how=\"inner\"                     # only keep matching rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = merged_train_df['category'].unique()\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for i, lbl in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'food', 'health and lifestyle', 'travel', 'sports',\n",
       "       'fashion', 'entertainment', 'mom and children', 'art', 'gaming'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_profile_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username2posts_test_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username2posts_train_preprocessed[\"deparmedya\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification_df.groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectorFactory.seed = 0\n",
    "def detect_language_distribution(texts):\n",
    "    lang_counts = Counter()\n",
    "    for text in texts:\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "            lang_counts[lang] += 1\n",
    "        except LangDetectException:\n",
    "            # Could not detect language (too short text or empty, etc.)\n",
    "            # print(\"Could not detect language for text:\", text[:50])\n",
    "            # print(\"Exception:\", e)\n",
    "            lang_counts['unknown'] += 1\n",
    "    return lang_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_emoji_content(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      0 -> if text has no emojis\n",
    "      1 -> if text has emojis AND other non-emoji characters\n",
    "      2 -> if text consists only of emojis\n",
    "    \"\"\"\n",
    "    # Find all potential emojis\n",
    "    found_emojis = regex.findall(r'\\p{Emoji}', text)\n",
    "    \n",
    "    # Filter out numbers and keycap symbols (#, *)\n",
    "    filtered_emojis = [e for e in found_emojis if e not in \"0123456789#*\"]\n",
    "\n",
    "    if not filtered_emojis:\n",
    "        # 0: No valid emojis found\n",
    "        return 0\n",
    "\n",
    "    # Check if text consists only of emojis (ignoring spaces)\n",
    "    stripped_text = text.strip()\n",
    "    emoji_text = ''.join(filtered_emojis)\n",
    "\n",
    "    if stripped_text == emoji_text:\n",
    "        # 2: Text consists only of emojis\n",
    "        return 2\n",
    "\n",
    "    # 1: Text contains both emojis and other characters\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "missing_captions = 0\n",
    "empty_captions = 0\n",
    "\n",
    "emoji_usage = [0,0,0]  # [no_emojis, mixed_text, only_emojis]\n",
    "emoji_usage_per_category = defaultdict(Counter)\n",
    "\n",
    "emojis = Counter()\n",
    "emojis_per_category = defaultdict(Counter)\n",
    "\n",
    "\n",
    "dates = Counter()\n",
    "\n",
    "for username, posts in username2posts_train_preprocessed.items():\n",
    "    for post in posts:\n",
    "        if \"caption\" in post and post[\"caption\"]:\n",
    "            captions.append({\"caption\": post[\"caption\"], \"class\": post[\"class\"], \"username\": username})\n",
    "            emojis_class = classify_emoji_content(post[\"caption\"])\n",
    "            emoji_usage[emojis_class] += 1\n",
    "            date = post[\"timestamp\"][5:10]\n",
    "            dates[date] += 1\n",
    "            emoji_usage_per_category[username2_category[username]][emojis_class] += 1\n",
    "            found_emojis = regex.findall(r'\\p{Emoji}', post[\"caption\"])\n",
    "            for e in found_emojis:\n",
    "                if e not in \"0123456789#*\":\n",
    "                    emojis[e] += 1\n",
    "                    emojis_per_category[username2_category[username]][e] += 1\n",
    "        else:\n",
    "            missing_captions += 1\n",
    "            if \"caption\" in post:\n",
    "                empty_captions += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of emoji content in captions\n",
    "labels = [\"No emojis\", \"Mixed text\", \"Only emojis\"]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, emoji_usage, color=[\"steelblue\", \"salmon\", \"seagreen\"])\n",
    "plt.title(\"Emoji Distribution\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Emojis\n",
    "emojis.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Emojis per Category\n",
    "for category, counter in emojis_per_category.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    print(counter.most_common(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of emoji content in captions per category\n",
    "# for category, emoji_counts in emojis_per_category.items():\n",
    "#     total = sum(emoji_counts.values())\n",
    "#     print(f\"Category: {category}, Total: {total}\")\n",
    "#     print(\"Emoji Distribution:\", {labels[i]: count/total for i, count in emoji_counts.items()})\n",
    "\n",
    "# Plot the distribution of emojis per category\n",
    "num_categories = len(emoji_usage_per_category)\n",
    "fig, axes = plt.subplots((num_categories + 2) // 3, 3, figsize=(15, 15))\n",
    "axes = axes.flatten()  # Flatten the axes array for easy indexing\n",
    "\n",
    "for i, (category, emoji_counts) in enumerate(emoji_usage_per_category.items()):\n",
    "    total = sum(emoji_counts.values())\n",
    "    ax = axes[i]\n",
    "    ax.bar(labels, [count/total for count in emoji_counts.values()], color=[\"steelblue\", \"salmon\", \"seagreen\"])\n",
    "    ax.set_title(f\"Category: {category}\")\n",
    "    ax.set_ylabel(\"Proportion\")\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of captions:\", len(captions) + missing_captions)\n",
    "print(\"Number of posts with missing captions:\", missing_captions)\n",
    "print(\"Number of posts with empty captions:\", empty_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language distribution in captions\n",
    "caption_lang_dist = detect_language_distribution([item[\"caption\"] for item in captions])\n",
    "caption_lang_dist_df = pd.DataFrame(\n",
    "        caption_lang_dist.items(), \n",
    "        columns=['language', 'count']\n",
    "    ).sort_values(by='count', ascending=False)\n",
    "\n",
    "# print(caption_lang_dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_lang_dist_df.plot.bar(x='language', y='count', legend=False, logy=True)\n",
    "plt.xlabel(\"Language\")\n",
    "plt.ylabel(\"Count (Log Scale)\")\n",
    "plt.title(\"Language Distribution of Captions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df = pd.DataFrame(list(dates.most_common(30)), columns=['Date', 'Count'])\n",
    "dates_df[\"Month\"] = dates_df[\"Date\"].str[:2]\n",
    "dates_df[\"Day\"] = dates_df[\"Date\"].str[3:]\n",
    "dates_df = dates_df.sort_values(by=[\"Month\", \"Day\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(dates_df['Date'], dates_df['Count'], s=100, color='b', alpha=0.7)\n",
    "plt.title(\"Most Common Dates in Posts\")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df = pd.DataFrame(list(dates.items()), columns=['Date', 'Count'])\n",
    "dates_df[\"Month\"] = dates_df[\"Date\"].str[:2]\n",
    "dates_df[\"Day\"] = dates_df[\"Date\"].str[3:]\n",
    "dates_df = dates_df.sort_values(by=[\"Month\", \"Day\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(dates_df['Date'], dates_df['Count'], s=100, color='b', alpha=0.7)\n",
    "plt.title(\"Date Distribution in Posts\")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks([])\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use merged_train_df to get the biographs and its category\n",
    "bio_cat_list = (\n",
    "    merged_train_df[['username','biography','category']]\n",
    "    .dropna(subset=['biography'])\n",
    "    .to_dict(orient='records')\n",
    ")\n",
    "\n",
    "for row in bio_cat_list:\n",
    "    processed_bio = preprocess_text(row['biography'])\n",
    "    captions.append({\n",
    "        \"caption\": processed_bio,\n",
    "        \"class\": row['category'],\n",
    "        \"username\": row['username']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_captions = pd.DataFrame(captions)\n",
    "df_train_captions[\"label_id\"] = df_train_captions[\"class\"].map(label2id)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_train_captions[[\"caption\", \"label_id\", \"username\"]])\n",
    "\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dbmdz/bert-base-turkish-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list)\n",
    ")\n",
    "\n",
    "# Optionally set model.config.id2label and label2id for readability\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"caption\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "def rename_label_column(examples):\n",
    "    examples[\"labels\"] = examples[\"label_id\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = tokenized_train_dataset.map(rename_label_column, batched=True)\n",
    "tokenized_eval_dataset = tokenized_eval_dataset.map(rename_label_column, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,  # if you have a separate eval, pass it as well\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and tokenizer locally\n",
    "save_directory = \"./saved_model\"\n",
    "trainer.save_model(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
